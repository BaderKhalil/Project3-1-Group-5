{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports &#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:10.308628Z",
     "iopub.status.busy": "2024-12-05T12:37:10.308226Z",
     "iopub.status.idle": "2024-12-05T12:37:17.357183Z",
     "shell.execute_reply": "2024-12-05T12:37:17.356512Z",
     "shell.execute_reply.started": "2024-12-05T12:37:10.308600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load Dataset &#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:17.359451Z",
     "iopub.status.busy": "2024-12-05T12:37:17.358900Z",
     "iopub.status.idle": "2024-12-05T12:37:17.483477Z",
     "shell.execute_reply": "2024-12-05T12:37:17.482616Z",
     "shell.execute_reply.started": "2024-12-05T12:37:17.359413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/kaggle/input/eedi-mining-misconceptions-in-mathematics\"\n",
    "EXTERNAL_DATA_PATH = \"/kaggle/input/eedi-external-dataset\"\n",
    "\n",
    "# train_df = pd.read_csv(f'{DATA_PATH}/train.csv', index_col='QuestionId')\n",
    "train_df = pd.read_csv(f'{EXTERNAL_DATA_PATH}/all_train.csv', index_col='QuestionId') #this contains the original dataset + an external dataset generated by a LLM\n",
    "test_df = pd.read_csv(f'{DATA_PATH}/test.csv')\n",
    "misconceptions_df = pd.read_csv(f'{DATA_PATH}/misconception_mapping.csv')\n",
    "\n",
    "pd.options.display.max_colwidth = 300\n",
    "display(train_df.head(5))\n",
    "pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preprocessing &#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:17.485001Z",
     "iopub.status.busy": "2024-12-05T12:37:17.484652Z",
     "iopub.status.idle": "2024-12-05T12:37:25.828819Z",
     "shell.execute_reply": "2024-12-05T12:37:25.827968Z",
     "shell.execute_reply.started": "2024-12-05T12:37:17.484963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean(example, columns):\n",
    "    \"\"\"\n",
    "    Cleans the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        columns: columns that will be cleaned\n",
    "\n",
    "    Returns: update example containing 'clean' columns\n",
    "\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        text = example[f'{col}']\n",
    "\n",
    "        # Empty text\n",
    "        if type(text) not in (str, np.str_) or text=='':\n",
    "            example[f'clean_{col}'] = ''\n",
    "            return example\n",
    "\n",
    "        # 'text' from the example can be of type numpy.str_, let's convert it to a python str\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # Clean the text\n",
    "        text = re.sub(\"\\\"\", \" \", text) # removes the \" from certain texts\n",
    "        text = re.sub(\"\\n\", \" \", text) # removes the multiple \"\\n\" \n",
    "        text = re.sub(r\"(\\\\\\w+)(\\W)\", r\" \\1 \\2\", text) # matches with the LaTeX commands like \"\\hline{}\",... and transforms them to \" \\hline {}\"\n",
    "        text = re.sub(r\"([\\(|\\{|\\[|\\|])\", r\" \\1\", text) # matches every opening parenthesis types and puts spaces on their left\n",
    "        text = re.sub(r\"([\\)|\\}|\\]])\", r\"\\1 \", text) # matches every closing parenthesis types and puts spaces on their right\n",
    "        text = re.sub(r\"\\\\(?![a-zA-Z])\", \" \", text) # removes every backslash that is not the start of a LaTeX command\n",
    "        text = re.sub(r\"\\( | \\)\", \"\", text) # removes the parentheses that appear sometimes from nowhere \n",
    "        text = re.sub(r\"\\[ | \\]\", \"\", text) # removes the parentheses that appear sometimes from nowhere\n",
    "        \n",
    "        text = re.sub(r\" +\", \" \", text) # cleans the double spaces made by above substitutions\n",
    "        # Update the example with the cleaned text\n",
    "        example[f'clean_{col}'] = text.strip()\n",
    "    return example\n",
    "\n",
    "# testing_data = {\n",
    "#     'QuestionText': [\"This is a question with a newline\\nin the middle\"],\n",
    "#     'AnswerAText': [\"Answer A\\nwith newline and \\\\table[test]\"],\n",
    "#     'AnswerBText': [\"Answer B\\nwith newline and \\hline(uwo)\"],\n",
    "#     'AnswerCText': [\"Answer C\\nwith newline and \\color{gold}\"],\n",
    "#     'AnswerDText': [\"Answer D\\nwith newline and \\\\begin{tabular}\"]\n",
    "# }\n",
    "# df = pd.DataFrame(testing_data)\n",
    "# df = df.apply(clean, axis = 1, columns = columns_to_clean)\n",
    "# display(df.head(1))\n",
    "\n",
    "columns_to_clean = ['QuestionText', 'AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText']\n",
    "train_df = train_df.apply(clean, axis = 1, columns = columns_to_clean)\n",
    "\n",
    "# Adjust column order\n",
    "new_order = ['ConstructId', 'ConstructName', 'SubjectId', 'SubjectName', 'CorrectAnswer']\n",
    "for col in columns_to_clean:\n",
    "    new_order.append(col)\n",
    "    new_order.append(f'clean_{col}')\n",
    "new_order.extend(['MisconceptionAId', 'MisconceptionBId', 'MisconceptionCId', 'MisconceptionDId', 'source', 'MisconceptionAName', 'MisconceptionBName', 'MisconceptionCName', 'MisconceptionDName', 'OriginalQuestionId'])\n",
    "train_df = train_df[new_order]\n",
    "\n",
    "\n",
    "display_train_df = train_df[['QuestionText', 'clean_QuestionText','AnswerAText', 'clean_AnswerAText', 'AnswerBText', 'clean_AnswerBText', 'AnswerCText', 'clean_AnswerCText', 'AnswerDText', 'clean_AnswerDText']]\n",
    "pd.options.display.max_colwidth = 300\n",
    "display(display_train_df.head(1))\n",
    "pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reshape Dataset For Training &#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:25.830892Z",
     "iopub.status.busy": "2024-12-05T12:37:25.830621Z",
     "iopub.status.idle": "2024-12-05T12:37:26.159691Z",
     "shell.execute_reply": "2024-12-05T12:37:26.158779Z",
     "shell.execute_reply.started": "2024-12-05T12:37:25.830867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_df columns: QuestionID, ConstructID, ConstructName, CorrectAnswer, SubjectId, SubjectName, QuestionText, Answer[A/B/C/D]Text, Misconception[A/B/C/D]Id\n",
    "\n",
    "reshaped_data = []\n",
    "for _, row in train_df.iterrows():\n",
    "    for answer, misconception_id in zip(\n",
    "        ['clean_AnswerAText', 'clean_AnswerBText', 'clean_AnswerCText', 'clean_AnswerDText'],\n",
    "        ['MisconceptionAId', 'MisconceptionBId', 'MisconceptionCId', 'MisconceptionDId']\n",
    "    ): # turn the data into a format where each datapoint (row) represents an answer choice (i.e there are now 4 datapoints for each question)\n",
    "        reshaped_data.append({\n",
    "            'QuestionText': row['clean_QuestionText'],\n",
    "            'AnswerText': row[answer],\n",
    "            'MisconceptionId': row[misconception_id],\n",
    "            'SubjectName': row['SubjectName'],\n",
    "            'ConstructName': row['ConstructName']\n",
    "        })\n",
    "\n",
    "reshaped_df = pd.DataFrame(reshaped_data)\n",
    "display(reshaped_df.head())\n",
    "\n",
    "# removed columns: QuestionId, ConstructId, CorrectAnswer, SubjectId\n",
    "# other changes: Answer[A/B/C/D]Text are now in separate datapoints along with their associated Misconception[A/B/C/D]Texts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TF-IDF & OneHot Encoding&#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:26.160821Z",
     "iopub.status.busy": "2024-12-05T12:37:26.160584Z",
     "iopub.status.idle": "2024-12-05T12:37:26.169473Z",
     "shell.execute_reply": "2024-12-05T12:37:26.168416Z",
     "shell.execute_reply.started": "2024-12-05T12:37:26.160799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# remove NaN values (dropping all datapoints that do not have misconceptions assigned to them)\n",
    "# P.S. that means we are also deleting all the rows (answer choices) that are correct\n",
    "# P.P.S. unless somehow there are correct answers that have misconceptions associated with them\n",
    "print(reshaped_df['MisconceptionId'].isnull().sum())  # 10582 NaN values yikes :/\n",
    "reshaped_df = reshaped_df.dropna(subset=['MisconceptionId'])\n",
    "print(reshaped_df['MisconceptionId'].isnull().sum())  # 0 now yippie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:26.170576Z",
     "iopub.status.busy": "2024-12-05T12:37:26.170302Z",
     "iopub.status.idle": "2024-12-05T12:37:26.281435Z",
     "shell.execute_reply": "2024-12-05T12:37:26.280620Z",
     "shell.execute_reply.started": "2024-12-05T12:37:26.170552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# use TF-IDF vectorizer for text data (5000 terms from QuestionText + AnswerText)\n",
    "vectorizer = TfidfVectorizer(max_features=5000) \n",
    "reshaped_df['CombinedText'] = reshaped_df['QuestionText'] + \" \" + reshaped_df['AnswerText']\n",
    "X_tfidf = vectorizer.fit_transform(reshaped_df['CombinedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:26.282930Z",
     "iopub.status.busy": "2024-12-05T12:37:26.282598Z",
     "iopub.status.idle": "2024-12-05T12:37:26.369131Z",
     "shell.execute_reply": "2024-12-05T12:37:26.368236Z",
     "shell.execute_reply.started": "2024-12-05T12:37:26.282889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# use One hot encoding for categorical data (create a \"column\" for each unique subject and construct and represent each row with 0 and 1)\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "categorical_features = encoder.fit_transform(reshaped_df[['SubjectName', 'ConstructName']])\n",
    "\n",
    "# Combine all features\n",
    "X = hstack([X_tfidf, categorical_features])  \n",
    "y = reshaped_df['MisconceptionId']\n",
    "\n",
    "print(X.shape) # print shape of X to see total number of features (should be 5000 from tfidf + the number of uniqueconstructs and subjects)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.5, random_state=42) #Split the data into training and testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest Training&#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:26.370490Z",
     "iopub.status.busy": "2024-12-05T12:37:26.370175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# y_pred = rf_classifier.predict(X)\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing&#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def map_at_25(y_true, y_pred_probs, top_k=25):\n",
    "    \n",
    "    map_25 = 0.0\n",
    "    for true_label, pred_prob in zip(y_true, y_pred_probs):\n",
    "        # Get top_k predictions\n",
    "        top_preds = np.argsort(pred_prob)[::-1][:top_k]\n",
    "        \n",
    "        if not true_label:\n",
    "            continue\n",
    "        \n",
    "        score = 0.0\n",
    "        hits = 0\n",
    "        for i, pred in enumerate(top_preds, start=1):\n",
    "            if pred == true_label:\n",
    "                hits += 1\n",
    "                score += hits / i  # Precision at i\n",
    "        \n",
    "        # Average Precision at 25\n",
    "        map_25 += score / min(1, top_k)\n",
    "    \n",
    "    return map_25 / len(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_val_pred_probs = rf_classifier.predict_proba(X_val)\n",
    "y_val_true = list(y_val)\n",
    "\n",
    "map25_score = map_at_25(y_val_true, y_val_pred_probs)\n",
    "print(f\"MAP@25 Score: {map25_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred_probs = rf_classifier.predict_proba(X_val)\n",
    "\n",
    "# print predictions\n",
    "for idx, (true_label, pred_prob) in enumerate(zip(y_val, y_pred_probs)):\n",
    "    # Get top 25 predictions and probabilities\n",
    "    top_preds = np.argsort(pred_prob)[::-1][:25]\n",
    "    top_probs = pred_prob[top_preds]\n",
    "    \n",
    "    # Check if true is within top 25\n",
    "    in_top_25 = true_label in top_preds\n",
    "    \n",
    "    print(f\"Example {idx + 1}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(\"Top 25 Predictions (Misconception ID: Probability):\")\n",
    "    for pred, prob in zip(top_preds, top_probs):\n",
    "        print(f\"ID {pred}: {prob:.4f}\")\n",
    "    print(f\"True Label in Top 25: {in_top_25}\\n\")\n",
    "\n",
    "    # Number of questions to print\n",
    "    if idx == 10:  \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 5688622,
     "sourceId": 9387960,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
