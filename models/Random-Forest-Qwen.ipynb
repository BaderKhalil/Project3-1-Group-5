{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports &#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -q -y accelerate transformers torch \n",
    "# !pip uninstall -q -y sentence-transformers\n",
    "# !pip install -q accelerate==0.21.0\n",
    "# !pip install -q transformers==4.24.0\n",
    "# !pip install -q pytorch\n",
    "# !pip install -q sentence-transformers\n",
    "# !pip show accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:10.308628Z",
     "iopub.status.busy": "2024-12-05T12:37:10.308226Z",
     "iopub.status.idle": "2024-12-05T12:37:17.357183Z",
     "shell.execute_reply": "2024-12-05T12:37:17.356512Z",
     "shell.execute_reply.started": "2024-12-05T12:37:10.308600Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\edoua\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 01-11 15:53:36 _custom_ops.py:19] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "INFO 01-11 15:53:36 importing.py:10] Triton not installed; certain GPU-related functions will not be available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-11 15:53:38,352\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "try:\n",
    "    import vllm\n",
    "except:\n",
    "    !pip install torchvision==0.19.1\n",
    "    !pip install vllm==0.6.3.post1\n",
    "    !pip install optimum==1.23.3 autoawq==0.2.7.post2 auto-gptq==0.7.1 bitsandbytes==0.44.1 peft==0.12.0\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "lora_path = kagglehub.model_download(\"anhvth226/2211-lora-14b/transformers/default\")\n",
    "qw14b_path = kagglehub.model_download(\"anhvth226/qw14b-awq/transformers/default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load Dataset &#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:17.359451Z",
     "iopub.status.busy": "2024-12-05T12:37:17.358900Z",
     "iopub.status.idle": "2024-12-05T12:37:17.483477Z",
     "shell.execute_reply": "2024-12-05T12:37:17.482616Z",
     "shell.execute_reply.started": "2024-12-05T12:37:17.359413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"datasets/eedi-mining-misconceptions-in-mathematics\"\n",
    "EXTERNAL_DATA_PATH = \"datasets/eedi-external-dataset\"\n",
    "\n",
    "# train_df = pd.read_csv(f'{DATA_PATH}/train.csv', index_col='QuestionId')\n",
    "df_train = pd.read_csv(f\"{DATA_PATH}/train.csv\").fillna(-1).sample(10, random_state=42).reset_index(drop=True)\n",
    "# train_df = pd.read_csv(f'{EXTERNAL_DATA_PATH}/all_train.csv', index_col='QuestionId') #this contains the original dataset + an external dataset generated by a LLM\n",
    "test_df = pd.read_csv(f'{DATA_PATH}/test.csv')\n",
    "misconceptions_df = pd.read_csv(f'{DATA_PATH}/misconception_mapping.csv')\n",
    "df_misconception_mapping = pd.read_csv(f\"{DATA_PATH}/misconception_mapping.csv\")\n",
    "\n",
    "pd.options.display.max_colwidth = 300\n",
    "# display(train_df.head(5))\n",
    "pd.options.display.max_colwidth = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preprocessing &#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:17.485001Z",
     "iopub.status.busy": "2024-12-05T12:37:17.484652Z",
     "iopub.status.idle": "2024-12-05T12:37:25.828819Z",
     "shell.execute_reply": "2024-12-05T12:37:25.827968Z",
     "shell.execute_reply.started": "2024-12-05T12:37:17.484963Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# testing_data = {\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#     'QuestionText': [\"This is a question with a newline\\nin the middle\"],\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#     'AnswerAText': [\"Answer A\\nwith newline and \\\\table[test]\"],\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# df = df.apply(clean, axis = 1, columns = columns_to_clean)\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# display(df.head(1))\u001b[39;00m\n\u001b[0;32m     48\u001b[0m columns_to_clean \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestionText\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswerAText\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswerBText\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswerCText\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswerDText\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 49\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mapply(clean, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, columns \u001b[38;5;241m=\u001b[39m columns_to_clean)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Adjust column order\u001b[39;00m\n\u001b[0;32m     52\u001b[0m new_order \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConstructId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConstructName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubjectId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubjectName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrectAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "def clean(example, columns):\n",
    "    \"\"\"\n",
    "    Cleans the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        columns: columns that will be cleaned\n",
    "\n",
    "    Returns: update example containing 'clean' columns\n",
    "\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        text = example[f'{col}']\n",
    "\n",
    "        # Empty text\n",
    "        if type(text) not in (str, np.str_) or text=='':\n",
    "            example[f'clean_{col}'] = ''\n",
    "            return example\n",
    "\n",
    "        # 'text' from the example can be of type numpy.str_, let's convert it to a python str\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # Clean the text\n",
    "        text = re.sub(\"\\\"\", \" \", text) # removes the \" from certain texts\n",
    "        text = re.sub(\"\\n\", \" \", text) # removes the multiple \"\\n\" \n",
    "        text = re.sub(r\"(\\\\\\w+)(\\W)\", r\" \\1 \\2\", text) # matches with the LaTeX commands like \"\\hline{}\",... and transforms them to \" \\hline {}\"\n",
    "        text = re.sub(r\"([\\(|\\{|\\[|\\|])\", r\" \\1\", text) # matches every opening parenthesis types and puts spaces on their left\n",
    "        text = re.sub(r\"([\\)|\\}|\\]])\", r\"\\1 \", text) # matches every closing parenthesis types and puts spaces on their right\n",
    "        text = re.sub(r\"\\\\(?![a-zA-Z])\", \" \", text) # removes every backslash that is not the start of a LaTeX command\n",
    "        text = re.sub(r\"\\( | \\)\", \"\", text) # removes the parentheses that appear sometimes from nowhere \n",
    "        text = re.sub(r\"\\[ | \\]\", \"\", text) # removes the parentheses that appear sometimes from nowhere\n",
    "        \n",
    "        text = re.sub(r\" +\", \" \", text) # cleans the double spaces made by above substitutions\n",
    "        # Update the example with the cleaned text\n",
    "        example[f'clean_{col}'] = text.strip()\n",
    "    return example\n",
    "\n",
    "# testing_data = {\n",
    "#     'QuestionText': [\"This is a question with a newline\\nin the middle\"],\n",
    "#     'AnswerAText': [\"Answer A\\nwith newline and \\\\table[test]\"],\n",
    "#     'AnswerBText': [\"Answer B\\nwith newline and \\hline(uwo)\"],\n",
    "#     'AnswerCText': [\"Answer C\\nwith newline and \\color{gold}\"],\n",
    "#     'AnswerDText': [\"Answer D\\nwith newline and \\\\begin{tabular}\"]\n",
    "# }\n",
    "# df = pd.DataFrame(testing_data)\n",
    "# df = df.apply(clean, axis = 1, columns = columns_to_clean)\n",
    "# display(df.head(1))\n",
    "\n",
    "columns_to_clean = ['QuestionText', 'AnswerAText', 'AnswerBText', 'AnswerCText', 'AnswerDText']\n",
    "train_df = train_df.apply(clean, axis = 1, columns = columns_to_clean)\n",
    "\n",
    "# Adjust column order\n",
    "new_order = ['ConstructId', 'ConstructName', 'SubjectId', 'SubjectName', 'CorrectAnswer']\n",
    "for col in columns_to_clean:\n",
    "    new_order.append(col)\n",
    "    new_order.append(f'clean_{col}')\n",
    "new_order.extend(['MisconceptionAId', 'MisconceptionBId', 'MisconceptionCId', 'MisconceptionDId'])\n",
    "train_df = train_df[new_order]\n",
    "\n",
    "\n",
    "display_train_df = train_df[['QuestionText', 'clean_QuestionText','AnswerAText', 'clean_AnswerAText', 'AnswerBText', 'clean_AnswerBText', 'AnswerCText', 'clean_AnswerCText', 'AnswerDText', 'clean_AnswerDText']]\n",
    "pd.options.display.max_colwidth = 300\n",
    "display(display_train_df.head(1))\n",
    "pd.options.display.max_colwidth = 50\n",
    "\n",
    "display(misconceptions_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reshape Dataset For Training &#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:25.830892Z",
     "iopub.status.busy": "2024-12-05T12:37:25.830621Z",
     "iopub.status.idle": "2024-12-05T12:37:26.159691Z",
     "shell.execute_reply": "2024-12-05T12:37:26.158779Z",
     "shell.execute_reply.started": "2024-12-05T12:37:25.830867Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>AnswerText</th>\n",
       "      <th>MisconceptionId</th>\n",
       "      <th>MisconceptionText</th>\n",
       "      <th>SubjectName</th>\n",
       "      <th>ConstructName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[\\r 3 \\times 2+4-5\\r \\r where do the brackets ...</td>\n",
       "      <td>3 \\times (2+4) -5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\\r 3 \\times 2+4-5\\r \\r where do the brackets ...</td>\n",
       "      <td>3 \\times 2+ (4-5)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[\\r 3 \\times 2+4-5\\r \\r where do the brackets ...</td>\n",
       "      <td>3 \\times (2+4-5)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\\r 3 \\times 2+4-5\\r \\r where do the brackets ...</td>\n",
       "      <td>does not need brackets</td>\n",
       "      <td>1672.0</td>\n",
       "      <td>Confuses the order of operations, believes add...</td>\n",
       "      <td>BIDMAS</td>\n",
       "      <td>Use the order of operations to carry out calcu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>simplify the following, if possible: \\frac {m^...</td>\n",
       "      <td>m+1</td>\n",
       "      <td>2142.0</td>\n",
       "      <td>Does not know that to factorise a quadratic ex...</td>\n",
       "      <td>Simplifying Algebraic Fractions</td>\n",
       "      <td>Simplify an algebraic fraction by factorising ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        QuestionText              AnswerText  \\\n",
       "0  [\\r 3 \\times 2+4-5\\r \\r where do the brackets ...       3 \\times (2+4) -5   \n",
       "1  [\\r 3 \\times 2+4-5\\r \\r where do the brackets ...       3 \\times 2+ (4-5)   \n",
       "2  [\\r 3 \\times 2+4-5\\r \\r where do the brackets ...        3 \\times (2+4-5)   \n",
       "3  [\\r 3 \\times 2+4-5\\r \\r where do the brackets ...  does not need brackets   \n",
       "4  simplify the following, if possible: \\frac {m^...                     m+1   \n",
       "\n",
       "   MisconceptionId                                  MisconceptionText  \\\n",
       "0              NaN                                                NaN   \n",
       "1              NaN                                                NaN   \n",
       "2              NaN                                                NaN   \n",
       "3           1672.0  Confuses the order of operations, believes add...   \n",
       "4           2142.0  Does not know that to factorise a quadratic ex...   \n",
       "\n",
       "                       SubjectName  \\\n",
       "0                           BIDMAS   \n",
       "1                           BIDMAS   \n",
       "2                           BIDMAS   \n",
       "3                           BIDMAS   \n",
       "4  Simplifying Algebraic Fractions   \n",
       "\n",
       "                                       ConstructName  \n",
       "0  Use the order of operations to carry out calcu...  \n",
       "1  Use the order of operations to carry out calcu...  \n",
       "2  Use the order of operations to carry out calcu...  \n",
       "3  Use the order of operations to carry out calcu...  \n",
       "4  Simplify an algebraic fraction by factorising ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_df columns: QuestionID, ConstructID, ConstructName, CorrectAnswer, SubjectId, SubjectName, QuestionText, Answer[A/B/C/D]Text, Misconception[A/B/C/D]Id\n",
    "\n",
    "reshaped_data = []\n",
    "for _, row in train_df.iterrows():\n",
    "    for answer, misconception_id in zip(\n",
    "        ['clean_AnswerAText', 'clean_AnswerBText', 'clean_AnswerCText', 'clean_AnswerDText'],\n",
    "        ['MisconceptionAId', 'MisconceptionBId', 'MisconceptionCId', 'MisconceptionDId']\n",
    "    ): # turn the data into a format where each datapoint (row) represents an answer choice (i.e there are now 4 datapoints for each question)\n",
    "        misc_id = int(row[misconception_id]) if not pd.isna(row[misconception_id]) else row[misconception_id]\n",
    "        reshaped_data.append({\n",
    "            'QuestionText': row['clean_QuestionText'],\n",
    "            'AnswerText': row[answer],\n",
    "            'MisconceptionId': misc_id,\n",
    "            'MisconceptionText': misconceptions_df.loc[misconceptions_df['MisconceptionId'] == misc_id, 'MisconceptionName'].values[0] if not pd.isna(misc_id) else misc_id,\n",
    "            'SubjectName': row['SubjectName'],\n",
    "            'ConstructName': row['ConstructName']\n",
    "        })\n",
    "\n",
    "reshaped_df = pd.DataFrame(reshaped_data)\n",
    "display(reshaped_df.head())\n",
    "\n",
    "# removed columns: QuestionId, ConstructId, CorrectAnswer, SubjectId\n",
    "# other changes: Answer[A/B/C/D]Text are now in separate datapoints along with their associated Misconception[A/B/C/D]Texts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sentence Embeddings & OneHot Encoding&#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:26.160821Z",
     "iopub.status.busy": "2024-12-05T12:37:26.160584Z",
     "iopub.status.idle": "2024-12-05T12:37:26.169473Z",
     "shell.execute_reply": "2024-12-05T12:37:26.168416Z",
     "shell.execute_reply.started": "2024-12-05T12:37:26.160799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10582\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# remove NaN values (dropping all datapoints that do not have misconceptions assigned to them)\n",
    "# P.S. that means we are also deleting all the rows (answer choices) that are correct\n",
    "# P.P.S. unless somehow there are correct answers that have misconceptions associated with them\n",
    "print(reshaped_df['MisconceptionId'].isnull().sum())  # 10582 NaN values yikes :/\n",
    "reshaped_df = reshaped_df.dropna(subset=['MisconceptionId'])\n",
    "print(reshaped_df['MisconceptionId'].isnull().sum())  # 0 now yippie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4370, 6)\n"
     ]
    }
   ],
   "source": [
    "print(reshaped_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:26.170576Z",
     "iopub.status.busy": "2024-12-05T12:37:26.170302Z",
     "iopub.status.idle": "2024-12-05T12:37:26.281435Z",
     "shell.execute_reply": "2024-12-05T12:37:26.280620Z",
     "shell.execute_reply.started": "2024-12-05T12:37:26.170552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_ret = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_INPUT_V3 = '{QUESTION}\\nCorrect answer: {CORRECT_ANSWER}\\nStudent wrong answer: {STUDENT_WRONG_ANSWER}'\n",
    "def format_input_v3(row, wrong_choice):\n",
    "\n",
    "    assert wrong_choice in \"ABCD\"\n",
    "    # Extract values from the row\n",
    "    question_text = row.get(\"QuestionText\", \"No question text provided\")\n",
    "    subject_name = row.get(\"SubjectName\", \"Unknown subject\")\n",
    "    construct_name = row.get(\"ConstructName\", \"Unknown construct\")\n",
    "    # Extract the correct and wrong answer text based on the choice\n",
    "    correct_answer = row.get(\"CorrectAnswer\", \"Unknown\")\n",
    "    assert wrong_choice != correct_answer\n",
    "    correct_answer_text = row.get(f\"Answer{correct_answer}Text\", \"No correct answer text available\")\n",
    "    wrong_answer_text = row.get(f\"Answer{wrong_choice}Text\", \"No wrong answer text available\")\n",
    "\n",
    "    # Construct the question format\n",
    "    formatted_question = f\"\"\"Question: {question_text}\n",
    "    \n",
    "SubjectName: {subject_name}\n",
    "ConstructName: {construct_name}\"\"\"\n",
    "\n",
    "    # Return the extracted data\n",
    "    ret = {\n",
    "        \"QUESTION\": formatted_question,\n",
    "        \"CORRECT_ANSWER\": correct_answer_text,\n",
    "        \"STUDENT_WRONG_ANSWER\": wrong_answer_text,\n",
    "        \"MISCONCEPTION_ID\": row.get('Misconception{wrong_choice}Id'),\n",
    "\n",
    "        'ConstructName': row['ConstructName'],\n",
    "        'SubjectName': row['SubjectName'],\n",
    "        'QuestionText': row['QuestionText'],\n",
    "        'CorrectAnswerText': correct_answer_text,\n",
    "        'IncorrectAnswerText': wrong_answer_text,\n",
    "    }\n",
    "    ret[\"PROMPT\"] = TEMPLATE_INPUT_V3.format(**ret)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "items = []\n",
    "target_ids = []\n",
    "for _, row in df_ret.iterrows():\n",
    "    for choice in ['A', 'B', 'C', 'D']:\n",
    "        if choice == row[\"CorrectAnswer\"]:\n",
    "            continue\n",
    "        # if not IS_SUBMISSION and row[f'Misconception{choice}Id'] == -1:\n",
    "        #     continue\n",
    "            \n",
    "        correct_col = f\"Answer{row['CorrectAnswer']}Text\"\n",
    "        item = {'QuestionId_Answer': '{}_{}'.format(row['QuestionId'], choice)}\n",
    "        item['Prompt'] = format_input_v3(row, choice)['PROMPT']\n",
    "\n",
    "        item['ConstructName'] = row['ConstructName']\n",
    "        item['SubjectName'] = row['SubjectName']\n",
    "        item['QuestionText'] = row['QuestionText']\n",
    "        correct_answer = row[\"CorrectAnswer\"]\n",
    "        item['CorrectAnswerText'] = row[f'Answer{correct_answer}Text']\n",
    "        item['IncorrectAnswerText'] = row[f'Answer{choice}Text']\n",
    "        \n",
    "        items.append(item)\n",
    "        target_ids.append(int(row.get(f'Misconception{choice}Id', -1)))\n",
    "        \n",
    "df_input = pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:26.282930Z",
     "iopub.status.busy": "2024-12-05T12:37:26.282598Z",
     "iopub.status.idle": "2024-12-05T12:37:26.369131Z",
     "shell.execute_reply": "2024-12-05T12:37:26.368236Z",
     "shell.execute_reply.started": "2024-12-05T12:37:26.282889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_detailed_instruct(task_description: str, query: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}'\n",
    "\n",
    "def get_detailed_example(task_description: str, query: str, response: str) -> str:\n",
    "    return f'<instruct>{task_description}\\n<query>{query}\\n<response>{response}'\n",
    "\n",
    "def get_new_queries(queries, query_max_len, examples_prefix, tokenizer):\n",
    "    inputs = tokenizer(\n",
    "        queries,\n",
    "        max_length=query_max_len - len(tokenizer('<s>', add_special_tokens=False)['input_ids']) - len(\n",
    "            tokenizer('\\n<response></s>', add_special_tokens=False)['input_ids']),\n",
    "        return_token_type_ids=False,\n",
    "        truncation=True,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    prefix_ids = tokenizer(examples_prefix, add_special_tokens=False)['input_ids']\n",
    "    suffix_ids = tokenizer('\\n<response>', add_special_tokens=False)['input_ids']\n",
    "    new_max_length = (len(prefix_ids) + len(suffix_ids) + query_max_len + 8) // 8 * 8 + 8\n",
    "    new_queries = tokenizer.batch_decode(inputs['input_ids'])\n",
    "    for i in range(len(new_queries)):\n",
    "        new_queries[i] = examples_prefix + new_queries[i] + '\\n<response>'\n",
    "    return new_max_length, new_queries\n",
    "task =  \"Given a math multiple-choice problem with a student's wrong answer, retrieve the math misconceptions\"\n",
    "queries = [\n",
    "    get_detailed_instruct(task, q) for q in df_input['Prompt']\n",
    "]\n",
    "documents = df_misconception_mapping['MisconceptionName'].tolist()\n",
    "query_max_len, doc_max_len = 1024, 256\n",
    "LORA_PATH = lora_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(qw14b_path)\n",
    "examples_prefix = ''\n",
    "new_query_max_len, new_queries = get_new_queries(queries, query_max_len, examples_prefix, tokenizer)\n",
    "\n",
    "\n",
    "import json\n",
    "with open('data.json', 'w') as f:\n",
    "    data = {'texts': new_queries+ documents}\n",
    "    f.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_embed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_embed.py\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import peft\n",
    "\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[\n",
    "            torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths\n",
    "        ]\n",
    "\n",
    "\n",
    "def get_embeddings_in_batches(model, tokenizer, texts, max_length, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        batch_dict = tokenizer(\n",
    "            batch_texts,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "        with torch.no_grad(), torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(**batch_dict)\n",
    "            batch_embeddings = last_token_pool(\n",
    "                outputs.last_hidden_state, batch_dict[\"attention_mask\"]\n",
    "            )\n",
    "            batch_embeddings = F.normalize(batch_embeddings, p=2, dim=1).cpu()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(base_model_path, lora_path, load_in_4bit=True):\n",
    "    model = AutoModel.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map=0,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        lora_path if lora_path else base_model_path\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    if lora_path:\n",
    "        model = peft.PeftModel.from_pretrained(model, lora_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    output_file = args.input_text.replace(\n",
    "        \".json\", \".pt.fold.{}.{}.embed\".format(*args.fold)\n",
    "    )\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Output file {output_file} already exists. Skipping...\")\n",
    "        return\n",
    "    model, tokenizer = load_model_and_tokenizer(\n",
    "        args.base_model, args.lora_path, load_in_4bit=args.load_in_4bit\n",
    "    )\n",
    "    texts = json.load(open(args.input_text))[\"texts\"][args.fold[0] :: args.fold[1]]\n",
    "    embeddings = get_embeddings_in_batches(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        texts,\n",
    "        max_length=MAX_LENGTH,\n",
    "        batch_size=4,\n",
    "    )\n",
    "    text2embeds = {text: emb for text, emb in zip(texts, embeddings)}\n",
    "    torch.save(text2embeds, output_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--base_model\",\n",
    "        type=str,\n",
    "        default=\"Qwen/Qwen2.5-7B\",\n",
    "        help=\"Path to the base model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Path to the LoRA model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_text\",\n",
    "        type=str,\n",
    "        default=\".cache/data.json\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--load_in_4bit\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Load model in 4-bit mode\",\n",
    "    )\n",
    "    parser.add_argument(\"--fold\", nargs=2, type=int, default=[0, 1])\n",
    "    args = parser.parse_args()\n",
    "    if not os.path.exists(args.lora_path):\n",
    "        args.lora_path = None\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sleep' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'sleep' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!sleep 1 & sleep 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "lora_path = '/kaggle/input/2211-lora-14b/transformers/default/1'\n",
    "cmd = f\"(CUDA_VISIBLE_DEVICES=0 python run_embed.py --base_model {qw14b_path} --lora_path {lora_path} --input_text data.json --fold 0 2) & (CUDA_VISIBLE_DEVICES=1 python run_embed.py --base_model {qw14b_path} --lora_path {lora_path} --input_text data.json --fold 1 2)\"\n",
    "import os\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m files \u001b[38;5;241m=\u001b[39m glob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.pt*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     files \u001b[38;5;241m=\u001b[39m glob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.pt*\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)    \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import time\n",
    "text_to_embed = {}\n",
    "files = glob('*.pt*')\n",
    "while len(files) != 2:\n",
    "    time.sleep(1)\n",
    "    files = glob('*.pt*')\n",
    "\n",
    "time.sleep(3)    \n",
    "for path in files:\n",
    "    print(path)\n",
    "    text_to_embed.update(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = torch.stack([text_to_embed[t] for t in new_queries])\n",
    "doc_embeddings = torch.stack([text_to_embed[t] for t in documents])\n",
    "query_embeddings.shape, doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eedi_11scores = query_embeddings @ doc_embeddings.T  # Shape: (M, N)\n",
    "sorted_indices = torch.argsort(eedi_11scores,1, descending=True)[:,:35].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input[\"MisconceptionId\"] = [\" \".join([str(x) for x in row]) for row in sorted_indices]\n",
    "df_input[[\"QuestionId_Answer\", \"MisconceptionId\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest Training&#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:37:26.370490Z",
     "iopub.status.busy": "2024-12-05T12:37:26.370175Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# y_pred = rf_classifier.predict(X)\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing&#8595;**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def map_at_25(y_true, y_pred_probs, top_k=25):\n",
    "    \n",
    "    map_25 = 0.0\n",
    "    for true_label, pred_prob in zip(y_true, y_pred_probs):\n",
    "        # Get top_k predictions\n",
    "        top_preds = np.argsort(pred_prob)[::-1][:top_k]\n",
    "        \n",
    "        if not true_label:\n",
    "            continue\n",
    "        \n",
    "        score = 0.0\n",
    "        hits = 0\n",
    "        for i, pred in enumerate(top_preds, start=1):\n",
    "            if pred == true_label:\n",
    "                hits += 1\n",
    "                score += hits / i  # Precision at i\n",
    "        \n",
    "        # Average Precision at 25\n",
    "        map_25 += score / min(1, top_k)\n",
    "    \n",
    "    return map_25 / len(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@25 Score: 0.0022291077409971054\n"
     ]
    }
   ],
   "source": [
    "y_val_pred_probs = rf_classifier.predict_proba(X_val)\n",
    "y_val_true = list(y_val)\n",
    "\n",
    "map25_score = map_at_25(y_val_true, y_val_pred_probs)\n",
    "print(f\"MAP@25 Score: {map25_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "True Label: 2030.0\n",
      "Top 25 Predictions (Misconception ID: Probability):\n",
      "True Label in Top 25: False\n",
      "\n",
      "Example 2\n",
      "True Label: 1794.0\n",
      "Top 25 Predictions (Misconception ID: Probability):\n",
      "True Label in Top 25: False\n",
      "\n",
      "Example 3\n",
      "True Label: 649.0\n",
      "Top 25 Predictions (Misconception ID: Probability):\n",
      "True Label in Top 25: False\n",
      "\n",
      "Example 4\n",
      "True Label: 2340.0\n",
      "Top 25 Predictions (Misconception ID: Probability):\n",
      "True Label in Top 25: False\n",
      "\n",
      "Example 5\n",
      "True Label: 1214.0\n",
      "Top 25 Predictions (Misconception ID: Probability):\n",
      "True Label in Top 25: False\n",
      "\n",
      "Example 6\n",
      "True Label: 617.0\n",
      "Top 25 Predictions (Misconception ID: Probability):\n",
      "True Label in Top 25: False\n",
      "\n",
      "Example 7\n",
      "True Label: 60.0\n",
      "Top 25 Predictions (Misconception ID: Probability):\n",
      "True Label in Top 25: False\n",
      "\n",
      "Example 8\n",
      "True Label: 1214.0\n",
      "Top 25 Predictions (Misconception ID: Probability):\n",
      "True Label in Top 25: False\n",
      "\n",
      "Example 9\n",
      "True Label: 1510.0\n",
      "Top 25 Predictions (Misconception ID: Probability):\n",
      "True Label in Top 25: False\n",
      "\n",
      "Example 10\n",
      "True Label: 1078.0\n",
      "Top 25 Predictions (Misconception ID: Probability):\n",
      "True Label in Top 25: False\n",
      "\n",
      "Example 11\n",
      "True Label: 583.0\n",
      "Top 25 Predictions (Misconception ID: Probability):\n",
      "True Label in Top 25: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_probs = rf_classifier.predict_proba(X_val)\n",
    "\n",
    "# print predictions\n",
    "for idx, (true_label, pred_prob) in enumerate(zip(y_val, y_pred_probs)):\n",
    "    # Get top 25 predictions and probabilities\n",
    "    top_preds = np.argsort(pred_prob)[::-1][:25]\n",
    "    top_probs = pred_prob[top_preds]\n",
    "    \n",
    "    # Check if true is within top 25\n",
    "    in_top_25 = true_label in top_preds\n",
    "    \n",
    "    print(f\"Example {idx + 1}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(\"Top 25 Predictions (Misconception ID: Probability):\")\n",
    "    for pred, prob in zip(top_preds, top_probs):\n",
    "        print(f\"ID {pred}: {prob:.4f}\")\n",
    "    print(f\"True Label in Top 25: {in_top_25}\\n\")\n",
    "\n",
    "    # Number of questions to print\n",
    "    if idx == 10:  \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9738540,
     "sourceId": 82695,
     "sourceType": "competition"
    },
    {
     "datasetId": 5688622,
     "sourceId": 9387960,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
